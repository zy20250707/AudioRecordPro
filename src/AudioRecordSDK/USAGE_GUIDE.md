# AudioRecord SDK ä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬è®¾ç½®

```swift
import Foundation

// è·å– SDK å®ä¾‹
let audioAPI = AudioAPI.shared

// è®¾ç½®å›è°ƒ
audioAPI.onRecordingComplete = { recording in
    print("å½•åˆ¶å®Œæˆ: \(recording.fileName)")
}
```

### 2. éº¦å…‹é£å½•åˆ¶

```swift
@MainActor
func startMicrophoneRecording() async {
    do {
        // åˆ›å»ºéº¦å…‹é£çº¦æŸ
        let constraints = createMicrophoneConstraints(
            echoCancellation: true,
            noiseSuppression: true
        )
        
        // è·å–åª’ä½“æµ
        let stream = try await audioAPI.getUserMedia(constraints: constraints)
        
        // å¼€å§‹å½•åˆ¶
        try audioAPI.startRecording(stream: stream)
        
        print("éº¦å…‹é£å½•åˆ¶å·²å¼€å§‹")
        
    } catch {
        print("å½•åˆ¶å¤±è´¥: \(error.localizedDescription)")
    }
}
```

### 3. æ··éŸ³å½•åˆ¶ (éº¦å…‹é£ + ç³»ç»ŸéŸ³é¢‘)

```swift
@MainActor
func startMixedRecording() async {
    do {
        // åˆ›å»ºæ··éŸ³çº¦æŸ
        let constraints = createMixedAudioConstraints(
            echoCancellation: true,
            noiseSuppression: false  // æ··éŸ³æ—¶å¯èƒ½ä¸éœ€è¦å™ªéŸ³æŠ‘åˆ¶
        )
        
        // è·å–åª’ä½“æµ
        let stream = try await audioAPI.getUserMedia(constraints: constraints)
        
        // å¼€å§‹å½•åˆ¶
        try audioAPI.startRecording(stream: stream)
        
        print("æ··éŸ³å½•åˆ¶å·²å¼€å§‹")
        
    } catch {
        print("æ··éŸ³å½•åˆ¶å¤±è´¥: \(error.localizedDescription)")
    }
}
```

### 4. åœæ­¢å½•åˆ¶

```swift
func stopRecording() {
    audioAPI.stopRecording()
    print("å½•åˆ¶å·²åœæ­¢")
}
```

## ğŸ“Š ç›‘æ§å’Œå›è°ƒ

### éŸ³é¢‘ç”µå¹³ç›‘æ§

```swift
audioAPI.onLevel = { level in
    // level èŒƒå›´: 0.0 - 1.0
    let percentage = Int(level * 100)
    print("éŸ³é¢‘ç”µå¹³: \(percentage)%")
    
    // æ›´æ–° UI
    DispatchQueue.main.async {
        self.levelMeter.value = level
    }
}
```

### çŠ¶æ€æ›´æ–°

```swift
audioAPI.onStatus = { status in
    print("çŠ¶æ€æ›´æ–°: \(status)")
    
    // æ›´æ–° UI çŠ¶æ€
    DispatchQueue.main.async {
        self.statusLabel.text = status
    }
}
```

### å½•åˆ¶å®Œæˆ

```swift
audioAPI.onRecordingComplete = { recording in
    print("å½•åˆ¶å®Œæˆ:")
    print("  æ–‡ä»¶å: \(recording.fileName)")
    print("  æ—¶é•¿: \(recording.formattedDuration)")
    print("  å¤§å°: \(recording.formattedFileSize)")
    print("  è·¯å¾„: \(recording.fileURL.path)")
    
    // æ’­æ”¾å½•åˆ¶çš„æ–‡ä»¶
    playRecording(recording)
}
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### æ£€æŸ¥å½•åˆ¶çŠ¶æ€

```swift
func checkRecordingStatus() {
    if audioAPI.isRecording {
        print("æ­£åœ¨å½•åˆ¶ä¸­...")
    } else {
        print("æœªåœ¨å½•åˆ¶")
    }
}
```

### æµä¿¡æ¯è·å–

```swift
func getStreamInfo(stream: AudioStream) {
    print("æµ ID: \(stream.id)")
    print("å½•åˆ¶æ¨¡å¼: \(stream.recordingMode)")
    print("æµçŠ¶æ€: \(stream.active ? "æ´»è·ƒ" : "éæ´»è·ƒ")")
    
    let tracks = stream.getAudioTracks()
    for (index, track) in tracks.enumerated() {
        print("è½¨é“ \(index + 1):")
        print("  ID: \(track.id)")
        print("  æ ‡ç­¾: \(track.label)")
        print("  å¯ç”¨: \(track.enabled)")
        print("  çŠ¶æ€: \(track.readyState)")
    }
}
```

## âš ï¸ é”™è¯¯å¤„ç†

### å®Œæ•´çš„é”™è¯¯å¤„ç†ç¤ºä¾‹

```swift
@MainActor
func startRecordingWithErrorHandling() async {
    do {
        let constraints = createMicrophoneConstraints()
        let stream = try await audioAPI.getUserMedia(constraints: constraints)
        try audioAPI.startRecording(stream: stream)
        
    } catch AudioError.microphonePermissionDenied {
        showAlert("è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­å…è®¸éº¦å…‹é£æƒé™")
        
    } catch AudioError.systemAudioPermissionDenied {
        showAlert("è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­å…è®¸ç³»ç»ŸéŸ³é¢‘æƒé™")
        
    } catch AudioError.deviceNotFound {
        showAlert("æœªæ‰¾åˆ°éŸ³é¢‘è®¾å¤‡ï¼Œè¯·æ£€æŸ¥è®¾å¤‡è¿æ¥")
        
    } catch AudioError.alreadyRecording {
        showAlert("å½•åˆ¶å·²åœ¨è¿›è¡Œä¸­ï¼Œè¯·å…ˆåœæ­¢å½“å‰å½•åˆ¶")
        
    } catch AudioError.notSupported(let feature) {
        showAlert("å½“å‰ç‰ˆæœ¬ä¸æ”¯æŒ: \(feature)")
        
    } catch {
        showAlert("å½•åˆ¶å¤±è´¥: \(error.localizedDescription)")
    }
}

func showAlert(_ message: String) {
    // æ˜¾ç¤ºé”™è¯¯æç¤º
    print("é”™è¯¯: \(message)")
}
```

## ğŸ¯ SwiftUI é›†æˆç¤ºä¾‹

```swift
import SwiftUI

@available(macOS 14.4, *)
struct AudioRecorderView: View {
    @State private var isRecording = false
    @State private var audioLevel: Float = 0.0
    @State private var statusMessage = "å‡†å¤‡å°±ç»ª"
    
    private let audioAPI = AudioAPI.shared
    
    var body: some View {
        VStack(spacing: 20) {
            Text("éŸ³é¢‘å½•åˆ¶å™¨")
                .font(.title)
            
            Text(statusMessage)
                .foregroundColor(isRecording ? .green : .primary)
            
            if isRecording {
                ProgressView(value: Double(audioLevel), in: 0...1)
                    .progressViewStyle(LinearProgressViewStyle())
            }
            
            HStack {
                Button("å¼€å§‹å½•åˆ¶") {
                    Task { await startRecording() }
                }
                .disabled(isRecording)
                
                Button("åœæ­¢å½•åˆ¶") {
                    stopRecording()
                }
                .disabled(!isRecording)
            }
        }
        .padding()
        .onAppear {
            setupCallbacks()
        }
    }
    
    private func setupCallbacks() {
        audioAPI.onLevel = { level in
            audioLevel = level
        }
        
        audioAPI.onStatus = { status in
            statusMessage = status
        }
        
        audioAPI.onRecordingComplete = { recording in
            statusMessage = "å½•åˆ¶å®Œæˆ: \(recording.fileName)"
            isRecording = false
        }
    }
    
    private func startRecording() async {
        do {
            let constraints = createMicrophoneConstraints()
            let stream = try await audioAPI.getUserMedia(constraints: constraints)
            try audioAPI.startRecording(stream: stream)
            isRecording = true
        } catch {
            statusMessage = "å½•åˆ¶å¤±è´¥: \(error.localizedDescription)"
        }
    }
    
    private func stopRecording() {
        audioAPI.stopRecording()
        isRecording = false
    }
}
```

## ğŸ” è°ƒè¯•å’Œæµ‹è¯•

### è¿è¡Œ SDK æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•
./scripts/test_sdk.sh

# è‡ªåŠ¨è¿è¡Œå®é™…å½•åˆ¶æµ‹è¯•
echo "y" | ./scripts/test_sdk.sh
```

### åœ¨åº”ç”¨ä¸­é›†æˆæµ‹è¯•

```swift
// åœ¨ AppDelegate ä¸­æ·»åŠ 
@available(macOS 14.4, *)
func applicationDidFinishLaunching(_ notification: Notification) {
    // è¿è¡Œå¿«é€Ÿæµ‹è¯•
    IntegratedSDKTest.runStartupTests()
    
    // å¼€å‘æ¨¡å¼ä¸‹è¿è¡Œå®Œæ•´æµ‹è¯•
    #if DEBUG
    IntegratedSDKTest.runDevelopmentTests()
    #endif
}
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. æƒé™ç®¡ç†
- åœ¨ä½¿ç”¨å‰æ£€æŸ¥æƒé™çŠ¶æ€
- æä¾›æ¸…æ™°çš„æƒé™è¯·æ±‚è¯´æ˜
- å¤„ç†æƒé™è¢«æ‹’ç»çš„æƒ…å†µ

### 2. é”™è¯¯å¤„ç†
- ä½¿ç”¨å®Œæ•´çš„ do-catch å—
- ä¸ºæ¯ç§é”™è¯¯ç±»å‹æä¾›ç”¨æˆ·å‹å¥½çš„æç¤º
- è®°å½•é”™è¯¯æ—¥å¿—ç”¨äºè°ƒè¯•

### 3. UI æ›´æ–°
- ä½¿ç”¨ `@MainActor` ç¡®ä¿ UI æ›´æ–°åœ¨ä¸»çº¿ç¨‹
- æä¾›å®æ—¶çš„å½•åˆ¶çŠ¶æ€åé¦ˆ
- æ˜¾ç¤ºéŸ³é¢‘ç”µå¹³æŒ‡ç¤ºå™¨

### 4. èµ„æºç®¡ç†
- åŠæ—¶åœæ­¢ä¸éœ€è¦çš„å½•åˆ¶
- åœ¨åº”ç”¨é€€å‡ºæ—¶æ¸…ç†èµ„æº
- ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [SDK README](README.md) - å®Œæ•´çš„ API æ–‡æ¡£
- [æµ‹è¯•ç»“æœ](Tests/TestResults.md) - æµ‹è¯•æŠ¥å‘Š
- [åŸºç¡€ç¤ºä¾‹](Examples/BasicUsage.swift) - ä»£ç ç¤ºä¾‹
- [é›†æˆç¤ºä¾‹](Examples/IntegrationExample.swift) - SwiftUI ç¤ºä¾‹

## ğŸ’¡ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©å½•åˆ¶æ ¼å¼ï¼Ÿ
A: SDK è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ ¼å¼ï¼ˆWAV PCM Float32ï¼‰ï¼Œç¡®ä¿é«˜è´¨é‡å½•åˆ¶ã€‚

### Q: å¯ä»¥åŒæ—¶å½•åˆ¶å¤šä¸ªæµå—ï¼Ÿ
A: å½“å‰ç‰ˆæœ¬ä¸æ”¯æŒåŒæ—¶å½•åˆ¶å¤šä¸ªæµï¼Œè¿™æ˜¯ MVP ç‰ˆæœ¬çš„é™åˆ¶ã€‚

### Q: å¦‚ä½•å¤„ç†å½•åˆ¶ä¸­æ–­ï¼Ÿ
A: SDK ä¼šè‡ªåŠ¨å¤„ç†ä¸­æ–­ï¼Œå¹¶é€šè¿‡ `onStatus` å›è°ƒé€šçŸ¥çŠ¶æ€å˜åŒ–ã€‚

### Q: æ”¯æŒå“ªäº›éŸ³é¢‘è®¾å¤‡ï¼Ÿ
A: æ”¯æŒæ‰€æœ‰ macOS å…¼å®¹çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡ï¼ŒåŒ…æ‹¬å†…ç½®éº¦å…‹é£å’Œå¤–æ¥è®¾å¤‡ã€‚

